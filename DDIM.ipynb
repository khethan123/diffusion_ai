{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f491f8d9-cd0f-4571-a54f-97d519b90c7c",
   "metadata": {
    "id": "aa3d1652"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "## <b>Denoising Diffusion Implicit Models - DDIM</b>\n",
    "\n",
    "What we are implementing here is an unconditional model; we are not performing class conditioning in this notebook, which will be addressed in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae4d9c-266e-4ed6-b308-992c5a1f1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU fastai fastcore datasets torcheval diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ebda3",
   "metadata": {
    "id": "ca2ebda3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "import fastcore.all as fc\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from diffusion_ai import *\n",
    "from torch.nn import init\n",
    "from torch import nn,optim\n",
    "from functools import partial\n",
    "from diffusers import UNet2DModel\n",
    "from fastcore.foundation import L\n",
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from torch.utils.data import DataLoader,default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVvtlInKFOAs",
   "metadata": {
    "id": "IVvtlInKFOAs"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "torch.manual_seed(1)\n",
    "logging.disable(logging.WARNING)\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "torch.set_printoptions(precision=4, linewidth=140, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d94365-4106-4f51-9ac1-f1f82035e40e",
   "metadata": {},
   "source": [
    "### <font face='monospace'><b>Loading the dataset and preprocessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b7613",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "644536ed8bdf4dd0b385146032d4786d"
     ]
    },
    "id": "001b7613",
    "outputId": "c3c040ea-d2cd-495a-ad00-b4e867a2df7b"
   },
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3596dce-be51-4185-b54b-2ef1461f2e33",
   "metadata": {
    "id": "b337c22b"
   },
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "bs = 256\n",
    "\n",
    "@inplace\n",
    "def transformi(batch):\n",
    "    # Resize and normalize images in the batch.\n",
    "    batch[xl] = [F.pad(TF.to_tensor(img), (2, 2, 2, 2)) * 2 - 1 for img in batch[xl]]\n",
    "\n",
    "# Apply transformations to the dataset\n",
    "transformed_ds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(transformed_ds, bs, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a092464-f6bc-4285-8132-2bf46bb690f7",
   "metadata": {
    "id": "b337c22b"
   },
   "outputs": [],
   "source": [
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb278dc-41f7-4c07-9703-96fac7592e1e",
   "metadata": {
    "id": "8a66c3f1"
   },
   "outputs": [],
   "source": [
    "# Load previously trained diffusion model\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x):\n",
    "        return super().forward(*x).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc12c8-fbae-407c-94cc-c962e1e33fca",
   "metadata": {
    "id": "8a66c3f1"
   },
   "outputs": [],
   "source": [
    "# Initialize the model for FashionMNIST\n",
    "model = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128), norm_num_groups=8)\n",
    "model = torch.load('models/fashion_ddpm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d47f30",
   "metadata": {
    "id": "b4d47f30"
   },
   "outputs": [],
   "source": [
    "# Load inference model for FID, KID\n",
    "inference_model = torch.load('models/inference.pkl')\n",
    "del inference_model[8]\n",
    "del inference_model[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df9ca7-843f-464f-9d1d-267879a41d78",
   "metadata": {
    "id": "b337c22b"
   },
   "outputs": [],
   "source": [
    "image_eval = ImageEval(inference_model, dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8210b9-f2ee-4541-a80b-dd635cf93730",
   "metadata": {
    "id": "aa3d1652"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### <b>Implementing DDIM</b>\n",
    "\n",
    "The main process which differentiates DDPM and DDIM is the sampling process, which removes noise.\n",
    "\n",
    "In the context of the DDIM scheduler, `eta` is a parameter that controls the weight of the noise added in each diffusion step.\n",
    "\n",
    "The value of `eta` can influence the amount of noise added at each step and therefore the overall quality of the generated samples. A higher `eta` will result in more noise being added, which could potentially lead to more diverse but less accurate samples. Conversely, a lower `eta` will result in less noise being added, which could lead to more accurate but less diverse samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abebc3-2f95-4e4a-b3ca-2035b792101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n",
    "    beta = torch.linspace(betamin, betamax, n_steps)\n",
    "    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n",
    "\n",
    "sc = linear_sched(betamax=0.01)\n",
    "abar = sc.abar\n",
    "sig = sc.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf48804",
   "metadata": {
    "id": "3cf48804"
   },
   "outputs": [],
   "source": [
    "def ddim_step(x_t, t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta):\n",
    "    # Perform a single DDIM step.\n",
    "    vari = ((bbar_t1 / bbar_t) * (1 - abar_t / abar_t1))\n",
    "    sig = vari.sqrt() * eta\n",
    "    x_0_hat = ((x_t - bbar_t.sqrt() * noise) / abar_t.sqrt())\n",
    "    x_t = abar_t1.sqrt() * x_0_hat + (bbar_t1 - sig**2).sqrt() * noise\n",
    "    if t > 0:\n",
    "        x_t += sig * torch.randn(x_t.shape).to(x_t)  # Add random noise\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f60450",
   "metadata": {
    "id": "e9f60450"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(f, model, size, num_steps, skip_steps=1, eta=1.):\n",
    "    # Generate samples using the DDIM scheduler\n",
    "    timesteps = list(reversed(range(0, num_steps, skip_steps)))\n",
    "    x_t = torch.randn(size).to(model.device)\n",
    "    preds = []\n",
    "    for i, t in enumerate(progress_bar(timesteps)):\n",
    "        abar_t1 = abar[timesteps[i + 1]] if t > 0 else torch.tensor(1)\n",
    "        noise = model((x_t, t))\n",
    "        x_t = f(x_t, t, noise, abar[t], abar_t1, 1 - abar[t], 1 - abar_t1, eta)\n",
    "        preds.append(x_t.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868e0ea",
   "metadata": {
    "id": "0868e0ea",
    "outputId": "f80a3832-070e-44b3-9adf-f1d4397feeb4"
   },
   "outputs": [],
   "source": [
    "# Define the size of the samples and generate them\n",
    "sample_size = (16, 1, 32, 32)\n",
    "samples = sample(ddim_step, model, sample_size, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e01c4",
   "metadata": {
    "id": "064e01c4",
    "outputId": "4dca1627-9f4c-4800-ad3a-78f6a9f0b5a8"
   },
   "outputs": [],
   "source": [
    "# Scale and show the images\n",
    "scaled_samples = (samples[-1] * 2)#.clamp(-1, 1)\n",
    "show_images(scaled_samples[:25], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914faf1f-2e07-480b-a020-97393595b698",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "Calculate FID, KID scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626177e",
   "metadata": {
    "id": "3626177e",
    "outputId": "9699f88b-194f-4a1e-f0e4-f22dc5a4f7f6"
   },
   "outputs": [],
   "source": [
    "image_eval.fid(scaled_samples),image_eval.kid(scaled_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19dedf6-fcc9-455b-8f4d-4fdef60df5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eval.fid(xb),image_eval.kid(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdd254",
   "metadata": {
    "id": "eacdd254"
   },
   "outputs": [],
   "source": [
    "clean_mem() # Free up some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8278b6-8fb4-431c-aa2c-90093425bc7f",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "That's it. Now if we don't want a pre-trained model, we can instead train another model using the below code and try the above steps again. Just see how fast DDIM works compared to DDPM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc985d64-a280-436f-bb8b-6500242e70c3",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### Compelete architecture for reimplementing the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9984c5-8a32-42b2-b1e6-7a6f1c1e2371",
   "metadata": {
    "id": "17a59c22"
   },
   "outputs": [],
   "source": [
    "# Define a linear schedule for DDPM\n",
    "def linear_schedule(beta_min=0.0001, beta_max=0.02, num_steps=1000):\n",
    "    beta = torch.linspace(beta_min, beta_max, num_steps)\n",
    "    return SimpleNamespace(alpha=1.-beta, alpha_bar=(1.-beta).cumprod(dim=0), sigma=beta.sqrt())\n",
    "\n",
    "schedule = linear_schedule(beta_max=0.01)\n",
    "alpha_bar = schedule.alpha_bar\n",
    "alpha = schedule.alpha\n",
    "sigma = schedule.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f755a8",
   "metadata": {
    "id": "28f755a8"
   },
   "outputs": [],
   "source": [
    "# Function to add noise to images\n",
    "def noisify(images, alpha_bar):\n",
    "    device = images.device\n",
    "    batch_size = len(images)\n",
    "    time_steps = torch.randint(0, 1000, (batch_size,), dtype=torch.long)\n",
    "    noise = torch.randn(images.shape, device=device)\n",
    "    alpha_bar_t = alpha_bar[time_steps].reshape(-1, 1, 1, 1).to(device)\n",
    "    noisy_images = alpha_bar_t.sqrt() * images + (1 - alpha_bar_t).sqrt() * noise\n",
    "    return (noisy_images, time_steps.to(device)), noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cc927",
   "metadata": {
    "id": "024cc927"
   },
   "outputs": [],
   "source": [
    "# Custom UNet model for DDPM\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x):\n",
    "        return super().forward(*x).sample\n",
    "\n",
    "# Initialize DDPM model\n",
    "def init_ddpm(model):\n",
    "    for o in model.down_blocks:\n",
    "        for p in o.resnets:\n",
    "            p.conv2.weight.data.zero_()\n",
    "            for p in fc.L(o.downsamplers): \n",
    "                init.orthogonal_(p.conv.weight)\n",
    "    for o in model.up_blocks:\n",
    "        for p in o.resnets: \n",
    "            p.conv2.weight.data.zero_()\n",
    "    model.conv_out.weight.data.zero_()\n",
    "\n",
    "# Collate function for DDPM\n",
    "def collate_ddpm(batch):\n",
    "    return noisify(default_collate(batch)[xl], alpha_bar)\n",
    "\n",
    "# Create dataloaders for DDPM\n",
    "def create_dataloader(dataset):\n",
    "    return DataLoader(dataset, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96356fb5",
   "metadata": {
    "id": "96356fb5"
   },
   "outputs": [],
   "source": [
    "# Create the data loader\n",
    "dls = DataLoaders(create_dataloader(transformed_ds['train']), create_dataloader(transformed_ds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08f4b3",
   "metadata": {
    "id": "5b08f4b3",
    "outputId": "8c542db8-86ba-4d54-da99-d8e7ba1c9310"
   },
   "outputs": [],
   "source": [
    "# Initialize the model for FashionMNIST\n",
    "model = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "init_ddpm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3b609",
   "metadata": {
    "id": "72c3b609"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LR = 1e-3\n",
    "EPOCHS = 1\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "total_steps = EPOCHS * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=LR, total_steps=total_steps)\n",
    "callbacks = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n",
    "\n",
    "# Note: MixedPrecision() callback uses GradScaler which needs GPU! or else it might crash\n",
    "\n",
    "# Create model\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=LR, cbs=callbacks, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8da91e",
   "metadata": {
    "id": "5a8da91e"
   },
   "outputs": [],
   "source": [
    "learn.fit(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9392db",
   "metadata": {
    "id": "0d9392db"
   },
   "outputs": [],
   "source": [
    "# DDPM sampler\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(model, size):\n",
    "    parameters = next(model.parameters())\n",
    "    generated_images = torch.randn(size).to(parameters)\n",
    "    predictions = []\n",
    "    for t in reversed(range(1000)):\n",
    "        time_batch = torch.full((generated_images.shape[0],), t, device=parameters.device, dtype=torch.long)\n",
    "        noise = (torch.randn(generated_images.shape) if t > 0 else torch.zeros(generated_images.shape)).to(parameters)\n",
    "        alpha_bar_t1 = alpha_bar[t - 1] if t > 0 else torch.tensor(1)\n",
    "        beta_bar_t = 1 - alpha_bar[t]\n",
    "        beta_bar_t1 = 1 - alpha_bar_t1\n",
    "        predicted_noise = model((generated_images, time_batch))\n",
    "        x0_hat = ((generated_images - beta_bar_t.sqrt() * predicted_noise) / alpha_bar[t].sqrt())\n",
    "        generated_images = x0_hat * alpha_bar_t1.sqrt() * (1 - alpha[t]) / beta_bar_t + generated_images * alpha[t].sqrt() * beta_bar_t1 / beta_bar_t + sigma[t] * noise\n",
    "        predictions.append(generated_images.float().cpu())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf2d57",
   "metadata": {
    "id": "0edf2d57",
    "outputId": "888168a6-a665-415d-b603-8f62b5875263"
   },
   "outputs": [],
   "source": [
    "# Sample images using DDPM\n",
    "samples = sample_ddpm(model, (3, 1, 32, 32))\n",
    "scaled_samples = (samples[-1] + 0.5).clamp(0, 1)\n",
    "show_images(scaled_samples[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11226b14-38d6-4255-9ffe-62f9dd9e3ddd",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "    \n",
    "**DDPM ↑**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38206ae9-cf60-4dc7-bde7-f95650cde270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sched(betamin=0.0001,betamax=0.02,n_steps=1000):\n",
    "    beta = torch.linspace(betamin, betamax, n_steps)\n",
    "    return SimpleNamespace(a=1.-beta, abar=(1.-beta).cumprod(dim=0), sig=beta.sqrt())\n",
    "\n",
    "# Initialize scheduler\n",
    "n_steps = 1000\n",
    "sc = linear_sched(betamax=0.01)\n",
    "abar = sc.abar\n",
    "sig = sc.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfN6WC3igJrR",
   "metadata": {
    "id": "XfN6WC3igJrR"
   },
   "outputs": [],
   "source": [
    "# Define DDIM step function\n",
    "def ddim_step(x_t, noise, alpha_bar_t, alpha_bar_t1, beta_bar_t, beta_bar_t1, eta, sigma):\n",
    "    sigma = ((beta_bar_t1 / beta_bar_t).sqrt() * (1 - alpha_bar_t / alpha_bar_t1).sqrt()) * eta\n",
    "    x0_hat = ((x_t - (1 - alpha_bar_t).sqrt() * noise) / alpha_bar_t.sqrt()).clamp(-1.5, 1.5)\n",
    "    sigma = torch.max(sigma, torch.tensor(0.0)) # Set to zero if very small or NaN\n",
    "    x_t = alpha_bar_t1.sqrt() * x0_hat + (beta_bar_t1 - sigma**2).sqrt() * noise\n",
    "    x_t += sigma * torch.randn(x_t.shape).to(x_t)\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mpWp_8bUgJrR",
   "metadata": {
    "id": "mpWp_8bUgJrR"
   },
   "outputs": [],
   "source": [
    "# Define sampling function\n",
    "@torch.no_grad()\n",
    "def sample_ddim(f, model, size, num_steps, skip_steps=1, eta=1.):\n",
    "    # Generate samples using the DDIM scheduler\n",
    "    timesteps = list(reversed(range(0, num_steps, skip_steps)))\n",
    "    x_t = torch.randn(size).to(model.device)\n",
    "    preds = []\n",
    "    for i, t in enumerate(progress_bar(timesteps)):\n",
    "        abar_t1 = abar[timesteps[i + 1]] if t > 0 else torch.tensor(1)\n",
    "        noise = model((x_t, t))\n",
    "        x_t = f(x_t, t, noise, abar[t], abar_t1, 1 - abar[t], 1 - abar_t1, eta)\n",
    "        preds.append(x_t.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfa4c2-a554-404d-8de8-d328ac1b4f9d",
   "metadata": {
    "id": "iWrTT7TugJrR"
   },
   "outputs": [],
   "source": [
    "# Sample images using DDIM\n",
    "sample_size = (256, 1, 32, 32)\n",
    "ddim_predictions = sample_ddim(ddim_step, model, sample_size, 100, eta=1.)\n",
    "s = (ddim_predictions[-1] * 2)  # Scale outputs to have range between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a56109-c906-41c0-b332-5cd2ed07c156",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "    \n",
    "**DDIM ↑**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e9c89-9bf8-4689-8d82-69097584aaf6",
   "metadata": {
    "id": "iWrTT7TugJrR"
   },
   "outputs": [],
   "source": [
    "show_images(s[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4yb_AWZGgJrR",
   "metadata": {
    "id": "4yb_AWZGgJrR",
    "outputId": "0398bca3-325d-4d2e-d78e-f80b6ea0f3a0"
   },
   "outputs": [],
   "source": [
    "image_eval.fid(s),image_eval.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tecZK2c_gJrR",
   "metadata": {
    "id": "tecZK2c_gJrR",
    "outputId": "3b02ddda-c008-41ac-80ad-1bf66011e1dd"
   },
   "outputs": [],
   "source": [
    "# Try for different number of steps.\n",
    "\n",
    "preds = sample_ddim(ddim_step, model, sample_size, steps=50, eta=1.)\n",
    "image_eval.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90076004-b11b-45ba-8343-f8951b55c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mem() # Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb449a-8313-43ed-9a18-ef2aa58c9346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
