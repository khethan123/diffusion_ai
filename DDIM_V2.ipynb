{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "## <b>Denoising Diffusion Implicit Models - DDIM</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we just see different variants of DDIM.\n",
    "\n",
    "DDIMs are a variant of diffusion models without noise (σ = 0), and DDPMs have noise (σ = 1). Any σ between 0 and 1 is an interpolation between a DDIM and DDPM.\n",
    "\n",
    "How can we add improvements to the DDPM/DDIM implementation? How about\n",
    "- the removal of the concept of an integral number of steps, making the process more continuous.\n",
    "- predicting the amount of noise in an image without passing the time step as input and modify the DDIM step to use the predicted alpha bar for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "1️⃣: Let's try implementing the above conditions into a model to obtain a variant of DDIM.\n",
    "\n",
    "- Implicit Use of Noise Levels: The below code relies on continuous representation of noise levels (sigma) to add and remove noise, which is analogous to using continuous time steps without explicitly handling them.\n",
    "\n",
    "\n",
    "- Noise Prediction: The model predicts the noise component directly from the noisy images using these noise levels. This approach abstracts away the explicit time steps by using the noise scale directly, allowing the model to work with continuous noise levels.\n",
    "\n",
    "- This makes the sampling process more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU fastai fastcore datasets torcheval diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b0oGPvVCFiN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import fastcore.all as fc\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from glob import glob\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from torch import nn,tensor\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from diffusers import UNet2DModel\n",
    "from fastcore.foundation import L\n",
    "from torch.optim import lr_scheduler\n",
    "from fastprogress import progress_bar\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "\n",
    "from diffusion_ai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable logging warnings\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# Set printing options and seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(1)\n",
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "plt.rcParams['image.cmap'] = 'gray_r'\n",
    "plt.rcParams['figure.dpi'] = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "83f26b9f7ee94c2d8c42b2a148de8608"
     ]
    },
    "id": "RMlYeHMVCFiP",
    "outputId": "fdb43e8a-6900-4980-f73a-1bc2f6edfc32"
   },
   "outputs": [],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "n_steps = 1000\n",
    "batch_size = 48\n",
    "sz = (48,1,32,32)\n",
    "name = \"fashion_mnist\"\n",
    "dataset = load_dataset(name)\n",
    "\n",
    "@inplace\n",
    "def transformi(batch): \n",
    "    batch['image'] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in batch['image']]\n",
    "\n",
    "transformed_dataset  = dataset.with_transform(transformi)\n",
    "data_loaders = DataLoaders.from_dd(transformed_dataset , batch_size, num_workers=4)\n",
    "\n",
    "dl = data_loaders.train\n",
    "xb,yb = b = next(iter(dl))\n",
    "\n",
    "# Evaluate the generated samples using FID and KID\n",
    "cmodel = torch.load('models/inference.pkl')\n",
    "del(cmodel[8])\n",
    "del(cmodel[7])\n",
    "\n",
    "ie = ImageEval(cmodel, data_loaders, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjC2N8Pk_otC"
   },
   "source": [
    "<font face='monospace'>\n",
    "Standard deviation describes how dispersed a set of data is. Choosing the right `σ` value for model initialization or regularization helps to achieve the lowest possible loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTIW7rOqCFiQ"
   },
   "outputs": [],
   "source": [
    "# data_std = xb.std()\n",
    "data_std = torch.tensor(0.66)  # standard deviation of our entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbswajErCFiQ"
   },
   "outputs": [],
   "source": [
    "# Calculate scaling coefficients for noise\n",
    "def calculate_scalings(sigma):\n",
    "    total_variance = sigma ** 2 + data_std ** 2\n",
    "    c_skip = data_std ** 2 / total_variance\n",
    "    c_out = sigma * data_std / total_variance.sqrt()\n",
    "    c_in = 1 / total_variance.sqrt()\n",
    "    return c_skip, c_out, c_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_skip, c_out, c_in = calculate_scalings(data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am1NkSNgCFiS"
   },
   "outputs": [],
   "source": [
    "# Function to add noise to images\n",
    "def noisify(images):\n",
    "    device = images.device\n",
    "    sigma = (torch.randn([len(images)]) * 1.2 - 1.2).exp().to(images).reshape(-1, 1, 1, 1)\n",
    "    # σ ~= 0.19 and we maintain this scale throughout to obtain\n",
    "    # unit data distribution when scaling the data.\n",
    "    noise = torch.randn_like(images, device=device)\n",
    "    c_skip, c_out, c_in = calculate_scalings(sigma)\n",
    "    noised_images = images + noise * sigma\n",
    "    targets = (images - c_skip * noised_images) / c_out\n",
    "    return (noised_images * c_in, sigma.squeeze()), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn-XSCIhCFiS"
   },
   "outputs": [],
   "source": [
    "# Custom collate function for adding noise to input images.\n",
    "def custom_collate(batch):\n",
    "    return noisify(default_collate(batch)['image'])\n",
    "\n",
    "# Create DataLoader with custom collate function\n",
    "def create_dataloader(dataset):\n",
    "    return DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fCCPpgMCFiS"
   },
   "outputs": [],
   "source": [
    "dataloaders = DataLoaders(create_dataloader(transformed_dataset['train']), create_dataloader(transformed_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX-huffJCFiS"
   },
   "outputs": [],
   "source": [
    "dl = dataloaders.train\n",
    "(noised_input,sig),target = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3BwLlDxCFiS",
    "outputId": "05fc5042-cbf3-4a99-dc72-5d1c220cb0c7"
   },
   "outputs": [],
   "source": [
    "show_images(noised_input[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UloIyRoSc2Ha"
   },
   "source": [
    "<font face='monospace'>\n",
    "The above image looks noisy because it's preconditioned, we added a bit of noise and since we also obtain the target, we know how much noise is added. We use this information to improve the model. Also, note that we do not use `label` from the downloaded dataset, becauseare not using `CLIP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfYEq4jVCFiS",
    "outputId": "4bb3b204-01d8-450c-b85a-0de9450beba0"
   },
   "outputs": [],
   "source": [
    "show_images(target[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgYP2cBIdqK2"
   },
   "source": [
    "<font face='monospace'>\n",
    "we can see that target images which were clean are not clean in noised_input and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uurIl2DCFiT"
   },
   "outputs": [],
   "source": [
    "# Function to denoise images\n",
    "def denoise_images(target, noised_images, c_skip, c_out):\n",
    "    return target * c_out + noised_images * c_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPjDUwdLfjnZ",
    "outputId": "4b759f37-e480-4bfb-9e8c-de196d128119"
   },
   "outputs": [],
   "source": [
    "show_images(denoise_images(target, noised_input/c_in, c_skip, c_out)[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rZ40GuLfn1L"
   },
   "source": [
    "<font face='monospace'>\n",
    "And the above is how our original images are without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDk8xg2iCFiT"
   },
   "outputs": [],
   "source": [
    "# Custom UNet model\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x):\n",
    "        return super().forward(*x).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSbckXx4CFiT"
   },
   "outputs": [],
   "source": [
    "# Initialize DDPM model\n",
    "def init_ddpm(model):\n",
    "    for o in model.down_blocks:\n",
    "        for p in o.resnets:\n",
    "            p.conv2.weight.data.zero_()\n",
    "            for p in fc.L(o.downsamplers): \n",
    "                init.orthogonal_(p.conv.weight)\n",
    "    for o in model.up_blocks:\n",
    "        for p in o.resnets: \n",
    "            p.conv2.weight.data.zero_()\n",
    "    model.conv_out.weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOeIf8FICFiT"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 1e-2\n",
    "EPOCHS = 25\n",
    "opt_function = partial(optim.Adam, eps=1e-5)\n",
    "total_steps = EPOCHS * len(data_loaders.train)\n",
    "scheduler = partial(lr_scheduler.OneCycleLR, max_lr=learning_rate, total_steps=total_steps)\n",
    "callbacks = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(scheduler)]\n",
    "\n",
    "# Create model, initialize it and create a Learner\n",
    "model = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 256), norm_num_groups=8)\n",
    "init_ddpm(model)\n",
    "learn = Learner(model, dataloaders, nn.MSELoss(), lr=learning_rate, cbs=callbacks, opt_func=opt_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Dz3bBenCFiT",
    "outputId": "28770d1a-f39c-47f5-8dd1-da19bd4d86dc"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "learn.fit(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIvAfcsKCFiT"
   },
   "outputs": [],
   "source": [
    "# This is the denoising model.\n",
    "torch.save(learn.model, 'models/fashion_karras.pkl')\n",
    "model = learn.model = torch.load('models/fashion_karras.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rhSJ4MpCFiT"
   },
   "outputs": [],
   "source": [
    "# # Perform denoising with the trained model\n",
    "# with torch.no_grad():\n",
    "#     sigma = sig.cuda().reshape(-1, 1, 1, 1)  # we use the sigma of the first batch\n",
    "#     c_skip, c_out, c_in = calculate_scalings(sigma)\n",
    "#     target_pred = learn.model((noised_input.cuda(), sigma.cuda()))\n",
    "#     x0_pred = denoise_images(target_pred, noised_input.cuda() / c_in, c_skip, c_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sigma = sig.reshape(-1, 1, 1, 1)  # we use the sigma of the first batch\n",
    "    c_skip, c_out, c_in = calculate_scalings(sigma)\n",
    "    target_pred = learn.model((noised_input, sig))\n",
    "    x0_pred = denoise_images(target_pred, noised_input / c_in, c_skip, c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ejYkuoZCFiT",
    "outputId": "8095e87f-e895-488a-a02a-f7c59ed1415f"
   },
   "outputs": [],
   "source": [
    "show_images(noised_input[:25], imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hudkE6vCFiU",
    "outputId": "4199383e-d01f-4793-86df-c98eb6ead97c"
   },
   "outputs": [],
   "source": [
    "show_images(x0_pred[:25].clamp(-1,1), imsize=1.5, titles=fc.map_ex(sig[:25], '{:.02f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5SDnjcUf0Rz"
   },
   "source": [
    "<font face='monospace'>\n",
    "The above is a model that predicts the amount of noise to be removed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d1HLgV-iGE-"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "2️⃣: `σ` formulation in the above process was as shown below:\n",
    "```python\n",
    "σ = []*1.2 - 1.2; # batch_size times\n",
    "σ = σ.exp()\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "Now let's try a different formulation that is used in \"karras\"\n",
    "\n",
    "$$\n",
    "\\sigma(n, \\sigma_{min}, \\sigma_{max}, \\rho) = \\left( \\sigma_{max} + \\text{ramp}(0, 1, n) \\times (\\sigma_{min} - \\sigma_{max}) \\right)^\\rho\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of sigma values to generate.\n",
    "- $\\rho$ controls the non-linear transition between $ \\sigma_{max} $ and $ \\sigma_{min} $.\n",
    "- $ \\text{ramp}(0, 1, n) $ creates a sequence of values that linearly interpolate between 0 and 1 over $ n $ steps.\n",
    "\n",
    "The final sequence of sigma values will have an additional zero appended at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJCbGOaUCFiZ"
   },
   "outputs": [],
   "source": [
    "# Generate sigmas using Karras noise scheduling.\n",
    "def sigmas_karras(n, sigma_min=0.01, sigma_max=80., rho=7., device='cpu'):\n",
    "    ramp = torch.linspace(0, 1, n)\n",
    "    min_inv_rho = sigma_min**(1/rho)\n",
    "    max_inv_rho = sigma_max**(1/rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho-max_inv_rho))**rho\n",
    "    return torch.cat([sigmas, tensor([0.])]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6stVTEP7CFiZ"
   },
   "outputs": [],
   "source": [
    "# Function to denoise images\n",
    "def denoise_images(model, x, sig):\n",
    "    c_skip,c_out,c_in = calculate_scalings(sig)\n",
    "    return model((x*c_in, sig))*c_out + x*c_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### <b>Euler Sampler</b>\n",
    "The main idea is to follow a path of increasing data fidelity while reducing noise, which is guided by a learned model.\n",
    "\n",
    "In the context of diffusion models, the Euler method is adapted to update the noisy image $x$ using the denoising model's predictions. \n",
    "\n",
    "Equation:\n",
    "\n",
    "$x_{i+1} = x_i + \\left(\\frac{x_i - \\text{denoised}}{\\sigma_i}\\right) (\\sigma_{i+1} - \\sigma_i)$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the current noisy image.\n",
    "- $\\text{denoised}$ is the model's prediction of the denoised image.\n",
    "- $\\sigma_i$ is the current noise level.\n",
    "- $\\sigma_{i+1}$ is the next noise level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTzKWQSOCFiZ"
   },
   "outputs": [],
   "source": [
    "# Euler sampler for updating the noisy image.\n",
    "@torch.no_grad()\n",
    "def euler_sample(x, sigs, i, model):\n",
    "    sig,sig2 = sigs[i],sigs[i+1]\n",
    "    denoised = denoise_images(model, x, sig)\n",
    "    return x + (x-denoised)/sig*(sig2-sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jETvOWllCFia"
   },
   "outputs": [],
   "source": [
    "# Generate samples using the euler sampler with karras noising.\n",
    "def sample(sampler, model, steps=100, sigma_max=80., **kwargs):\n",
    "    preds = []\n",
    "    x = torch.randn(sz).to(model.device)*sigma_max\n",
    "    sigs = sigmas_karras(steps, device=model.device, sigma_max=sigma_max)\n",
    "    for i in progress_bar(range(len(sigs)-1)):\n",
    "        x = sampler(x, sigs, i, model, **kwargs)\n",
    "        preds.append(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8mKsLYiCFia",
    "outputId": "278adb46-7296-42a8-d2bb-e0daaba8813c"
   },
   "outputs": [],
   "source": [
    "preds = sample(euler_sample, model, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeWu8H_TCFia",
    "outputId": "c20aee23-3366-47c7-aa18-bb71510e0642"
   },
   "outputs": [],
   "source": [
    "s = preds[-1]\n",
    "s.min(),s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFeHxWv8CFia",
    "outputId": "20f168cf-c40c-447e-ceab-a858e4f2b04a"
   },
   "outputs": [],
   "source": [
    "show_images(s[:25].clamp(-1,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSbsBtR7CFia",
    "outputId": "16af3a76-f2e9-4cb9-eb9a-0d02da0a6fca"
   },
   "outputs": [],
   "source": [
    "# euler 100\n",
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYpWaZtpCFib",
    "outputId": "a222ec40-6015-4fd4-e366-2c5b851e530a"
   },
   "outputs": [],
   "source": [
    "# reals\n",
    "ie.fid(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "We can use many more different types of samplers like `Heun`, `Euler Ancestral`, `Linear Multistep Coefficient`, etc. All mostly produce similar results; The denoising/update step that they use is a bit different. That's all!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
