{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5025e1e5-81fb-4591-b1b0-fa525822deb6",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "## <b>Denoising Diffusion Probabilistic Models - DDPM</b>\n",
    "\n",
    "Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9beded-77bb-405f-9f13-82b1e0fab3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU fastai fastcore accelerate datasets torcheval diffusers ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b4d50-4b80-43dc-823a-7780c9e3961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from torch import nn,optim\n",
    "from fastcore.all import *\n",
    "from functools import partial\n",
    "from diffusers import UNet2DModel\n",
    "from fastcore.foundation import L\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "\n",
    "from diffusion_ai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb1eac-1eb6-4b42-8101-1637cff6f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "plt.rcParams['animation.writer'] = 'ffmpeg'\n",
    "plt.rcParams['image.cmap'] = 'gray_r'\n",
    "logging.disable(logging.WARNING)\n",
    "torch.manual_seed(1)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8364c-a996-4d7d-9af3-dcf9fc3aa22f",
   "metadata": {},
   "source": [
    "### <font face='monospace'><b>Loading the dataset and preprocessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458dd67-f12d-4673-b609-125054b73af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_KEY = 'image'\n",
    "LABEL_KEY = 'label'\n",
    "DATASET_NAME = \"fashion_mnist\"\n",
    "BATCH_SIZE = 128\n",
    "LR = 4e-3\n",
    "EPOCHS = 5\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "@inplace\n",
    "def transformi(batch):\n",
    "    \"\"\"\n",
    "    Transform function to resize and normalize images inplace in the batch.\n",
    "    \"\"\"\n",
    "    batch[IMAGE_KEY] = [TF.resize(TF.to_tensor(image), (32, 32)) * 2 - 1 for image in batch[IMAGE_KEY]]\n",
    "\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transformi)\n",
    "data_loaders = DataLoaders.from_dd(transformed_dataset, BATCH_SIZE, num_workers=4)\n",
    "print(\"DataLoaders for Fashion MNIST dataset created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de668a34-3832-4cbb-912a-963d2f7eb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data_loaders.train\n",
    "xb,yb = next(iter(dt))\n",
    "show_images(xb[:16], imsize=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8e827-4713-4e80-b0a5-2d70c52c81a0",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### <b>Training - easy with a callback!</b>\n",
    "DDPM is trained quite simply in a few steps:\n",
    "1. randomly select some timesteps in an iterative noising process (0, ... T).\n",
    "2. Add noise (n) corresponding to this timestep to the original image (n ∝ t : 0 <= t<= T, n ∈ N(0, 1)). For increasing timesteps, the variance of the noise increases.\n",
    "3. Pass in this noisy image and the timestep to our model\n",
    "4. Model is trained with MSE loss between the model output and the amount of noise added to the image at the timestep.\n",
    "<br>\n",
    "\n",
    "We will implement this in a callback. After training, we need to sample from this model. This is an iterative denoising process starting from pure noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f4500-190b-4b90-8e30-44ad13b41a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback to facilitate distributed training \n",
    "# with mixed precision using Accelerate.\n",
    "class AccelerateCB(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.acc = Accelerator(mixed_precision=mixed_precision)\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        \"\"\"\n",
    "        Prepare the model, optimizer, and dataloaders for training.\n",
    "        \"\"\"\n",
    "        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n",
    "            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n",
    "\n",
    "    def backward(self, learn): \n",
    "        \"\"\"\n",
    "        Perform backpropagation with mixed precision.\n",
    "        \"\"\"\n",
    "        self.acc.backward(learn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d689be-f5e9-49b4-80ed-809a408a46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(x0, alpha_bar):\n",
    "    \"\"\"\n",
    "    Add noise to the input images based on the given alpha_bar schedule.\n",
    "\n",
    "    Args:\n",
    "        x0 (tensor): The original clean images.\n",
    "        alpha_bar (tensor): The cumulative product of alphas used for noising.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the noised images and the corresponding time steps.\n",
    "    \"\"\"\n",
    "    device = x0.device\n",
    "    n = len(x0)  # returns the first dimension [b, c, h, w] -> b\n",
    "    t = torch.randint(0, 1000, (n,), dtype=torch.long).to(device)\n",
    "    epsilon = torch.randn(x0.shape, device=device)\n",
    "    alpha_bar_t = alpha_bar[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = alpha_bar_t.sqrt() * x0 + (1 - alpha_bar_t).sqrt() * epsilon\n",
    "    return (xt, t.to(device)), epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b514e-7478-45ce-bacc-50d2580d7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for Denoising Diffusion Probabilistic Models (DDPM)\n",
    "class DDPMCB(Callback):\n",
    "    \"\"\"\n",
    "    Callback to handle the training process for DDPMs.\n",
    "    \n",
    "    Attributes:\n",
    "        order (int): Execution order of the callback relative to other callbacks.\n",
    "        n_steps (int): Number of diffusion steps.\n",
    "        beta_min (float): Minimum beta value for the diffusion process.\n",
    "        beta_max (float): Maximum beta value for the diffusion process.\n",
    "        beta (tensor): Linearly spaced beta values.\n",
    "        alpha (tensor): Alpha values derived from beta.\n",
    "        alpha_bar (tensor): Cumulative product of alpha values.\n",
    "        sigma (tensor): Standard deviation values derived from beta.\n",
    "    \"\"\"\n",
    "    order = DeviceCB.order + 1\n",
    "    def __init__(self, n_steps, beta_min, beta_max):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.beta = torch.linspace(self.beta_min, self.beta_max, self.n_steps)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sigma = self.beta.sqrt()\n",
    "        \n",
    "    def before_batch(self, learn):\n",
    "        \"\"\"\n",
    "        Apply noise to the batch before each training iteration.\n",
    "        \"\"\"\n",
    "        learn.batch = noisify(learn.batch[0], self.alpha_bar)\n",
    "    \n",
    "    def sample(self, model, size):\n",
    "        \"\"\"\n",
    "        Generate samples from the trained DDPM model.\n",
    "        \"\"\"\n",
    "        return sample(model, size, self.alpha, self.alpha_bar, self.sigma, self.n_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9c963-1b27-4ef5-aa85-ea6df9430bac",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### <b>Sampling</b>\n",
    "The bellow `sample` function is a custom function that is different from the conventional DDPM sampler because it skips most of the sampling steps. And does not affect the output generation much. \n",
    "\n",
    "   - The `sample_at` condition `(t + 101) % ((t + 101) // 100) == 0` allows the sampler to skip unnecessary calculations for most timesteps.\n",
    "   - Instead of storing predictions at every timestep, the function only stores predictions at `sample_at` timesteps in the `preds` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ecf06-3f59-47ce-849a-476204d2e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, sz, alpha, alphabar, sigma, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Generate samples from a trained DDPM model.\n",
    "    It samples faster because of the `sample_at` condition.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained DDPM model.\n",
    "        sz (tuple): The size of the samples to generate.\n",
    "        alpha (torch.Tensor): Alpha values derived from beta.\n",
    "        alphabar (torch.Tensor): Cumulative product of alpha values.\n",
    "        sigma (torch.Tensor): Standard deviation values derived from beta.\n",
    "        n_steps (int): Number of diffusion steps.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated samples at various timesteps.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    x_t = torch.randn(sz, device=device)\n",
    "    sample_at = {t for t in range(n_steps) if (t + 101) % ((t + 101) // 100) == 0}\n",
    "    preds = []\n",
    "    noise = None\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        z = torch.randn(x_t.shape, device=device) if t > 0 else torch.zeros(x_t.shape, device=device)\n",
    "\n",
    "        alpha_t1 = alphabar[t-1] if t > 0 else torch.tensor(1.0).to(device)\n",
    "        beta_bar_t = 1 - alphabar[t]\n",
    "        beta_bar_t1 = 1 - alpha_t1\n",
    "\n",
    "        # Predict noise\n",
    "        if t in sample_at or noise is None:\n",
    "            noise = model((x_t, t_batch))\n",
    "\n",
    "        # Estimate the original clean image\n",
    "        x_0_hat = ((x_t - beta_bar_t.sqrt() * noise) / alphabar[t].sqrt()).clamp(-1, 1)\n",
    "        \n",
    "        # Calculate coefficients for combining x_0_hat and x_t\n",
    "        x0_coeff = alpha_t1.sqrt() * (1 - alpha[t]) / beta_bar_t\n",
    "        xt_coeff = alpha[t].sqrt() * beta_bar_t1 / beta_bar_t\n",
    "        \n",
    "        # Update x_t for the next timestep\n",
    "        x_t = x_0_hat * x0_coeff + x_t * xt_coeff + sigma[t] * z\n",
    "        \n",
    "        # Store intermediate results at specified timesteps\n",
    "        if t in sample_at:\n",
    "            preds.append(x_t.float().cpu())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea4e21-61c1-4b95-8101-bec5eb87bbe6",
   "metadata": {},
   "source": [
    "<font face='monospace'>**Let's use the predefined UNET model from diffusers library to predict the noise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d419c4-e27d-42f0-9091-f3571fd086b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DDPM model\n",
    "\n",
    "def init_ddpm(model):\n",
    "    for o in model.down_blocks:\n",
    "        for p in o.resnets:\n",
    "            p.conv2.weight.data.zero_()\n",
    "            for p in L(o.downsamplers): \n",
    "                init.orthogonal_(p.conv.weight)\n",
    "    for o in model.up_blocks:\n",
    "        for p in o.resnets: \n",
    "            p.conv2.weight.data.zero_()\n",
    "    model.conv_out.weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3af624-0dad-47c8-930a-78f7277e7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x):\n",
    "      return super().forward(*x).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16d170-0330-4181-afa8-413250a03205",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = EPOCHS * len(data_loaders.train)\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=LR, total_steps=tmax)\n",
    "ddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\n",
    "model = UNet(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128), norm_num_groups=8)\n",
    "init_ddpm(model)\n",
    "cbs = [ddpm_cb,\n",
    "        DeviceCB(), \n",
    "        ProgressCB(plot=True), \n",
    "        MetricsCB(), \n",
    "        BatchSchedCB(sched),\n",
    "        AccelerateCB()]\n",
    "learn = Learner(model, data_loaders, nn.MSELoss(), lr=LR, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bed925-d74b-4af7-9773-eaa4c7929f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3fce0-029a-4499-9b47-13b173a4c8bd",
   "metadata": {},
   "source": [
    "---\n",
    "<font face='monospace'>Let's sample from our model and see how the generated images are, also let's post-process them and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897e3bd-952d-41c7-bd1c-a7ce87f70e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.linspace(0.0001, 0.02, 1000)\n",
    "alpha = 1.0 - beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2005e-37fc-4dc7-82d5-25ef7e588802",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(learn.model, (1, 1, 32, 32), alpha, alphabar, sigma, 1000)\n",
    "s = samples[-1]*2 - 1\n",
    "show_images(s[:16], figsize=(4,4), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe90b5-e854-484f-b2a1-8bc010e1dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = Path('models')\n",
    "model_path.mkdir(exist_ok=True)\n",
    "torch.save(learn.model, model_path/'fashion_ddpm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e4a87-be40-448c-8f71-a59620f3d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "learn.model = torch.load(model_path/'fashion_ddpm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26442481-c166-41b1-9e03-120ccb56353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "\n",
    "# Let's visualize the sampling process\n",
    "def getImageFromList(x):\n",
    "    return s[x][0]\n",
    "\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ims = []\n",
    "for i in range(len(s)):\n",
    "    im = plt.imshow(getImageFromList(i), animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "plt.close()\n",
    "\n",
    "# Show the animation\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b89e2-246a-4a02-980c-a28e7ba34173",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "    \n",
    "<b>NOTE:</b> Don't forget to install ffmpeg in your operating system. If you are using conda the try `conda install conda-forge::ffmpeg` in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebebfd-07c1-45bc-b06a-64696d9fcb6f",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### **FID & KID**\n",
    "\n",
    "`FID` - It calculates the distance between feature vectors calculated for real and generated images.\n",
    "\n",
    "`KID` - It measures the squared Maximum Mean Discrepancy (MMD) between the Inception representations of the real and generated samples. MMD is a measure of the distance between two probability distributions. It's calculated using a kernel function, which is a measure of similarity between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0942895-4ed5-4fcd-9a63-45c277ce9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.linspace(0.0001, 0.02, 1000)\n",
    "alpha = 1.0 - beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5162d-fe08-43ba-815b-eee0fd74eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained diffusion model\n",
    "smodel = torch.load('models/fashion_ddpm.pkl')\n",
    "\n",
    "# Function to sample from the diffusion model\n",
    "@torch.no_grad()\n",
    "def sample(model, size, alpha, alphabar, sigma, n_steps):\n",
    "    device = next(model.parameters()).device\n",
    "    x_t = torch.randn(size, device=device)\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        z = torch.randn(x_t.shape).to(device) if t > 0 else torch.zeros(x_t.shape).to(device)\n",
    "        alphabar_t1 = alphabar[t-1] if t > 0 else torch.tensor(1.0, device=device)\n",
    "        bbar_t = 1 - alphabar[t]\n",
    "        bbar_t1 = 1 - alphabar_t1\n",
    "        x0_hat = (x_t - bbar_t.sqrt() * model((x_t, t_batch))) / alphabar[t].sqrt()\n",
    "        x_t = x0_hat * alphabar_t1.sqrt() * (1 - alpha[t]) / bbar_t + x_t * alpha[t].sqrt() * bbar_t1 / bbar_t + sigma[t] * z\n",
    "        preds.append(x0_hat.cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e884d28-8691-46a0-88f5-3f7646cc6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the diffusion model\n",
    "samples = sample(smodel, (128, 1, 32, 32), alpha, alphabar, sigma, 1000)\n",
    "s = samples[-1] * 2 - 1\n",
    "show_images(s[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ab2ae-7da5-4b08-90c9-a6b7b489bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi2(batch):\n",
    "    batch['image'] = [F.pad(TF.to_tensor(img), (2, 2, 2, 2)) * 2 - 1 for img in batch['image']]\n",
    "\n",
    "tds = dataset.with_transform(transformi2)\n",
    "dls = DataLoaders.from_dd(tds, BATCH_SIZE, num_workers=4)\n",
    "\n",
    "# Load the pre-trained CNN model for evaluation\n",
    "cmodel = torch.load('models/inference.pkl')\n",
    "del cmodel[8]  # these are linear and probability layers which we don't need\n",
    "del cmodel[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68daed-5ada-40b4-aa49-9904e576e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ImageEval object which takes the mean and\n",
    "# covariance of features at the last layer\n",
    "# and calculate FID and KID\n",
    "ie = ImageEval(cmodel, dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617eb50-c482-42c3-a109-67f52d78040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_generated = ie.fid(s)\n",
    "fid_original = ie.fid(xb * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0c1f4-0058-4fe2-bfb0-295396f4616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kid_generated = ie.kid(s)\n",
    "kid_original = ie.kid(xb*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d5469-5171-4e08-a7ce-9e1a834265be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FID of generated images: {fid_generated}\")\n",
    "print(f\"KID of generated images: {kid_generated}\")\n",
    "print()\n",
    "print(f\"FID of original images: {fid_original}\")\n",
    "print(f\"KID of original images: {kid_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b6447-4596-4ebd-8b75-4c6eca3ec9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e650296-63c2-4bf8-a269-4973c6e54981",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43590893-4a44-41a0-9c7d-e6856e90f2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
