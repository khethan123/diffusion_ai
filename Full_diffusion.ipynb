{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nHACryQ1Pri"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "## <b>FULL DIFFUSION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU fastai fastcore datasets torcheval diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb3Pu52c1Prk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import logging\n",
    "import fastcore.all as fc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from torch import nn, optim\n",
    "from scipy import integrate\n",
    "from functools import partial, wraps\n",
    "from datasets import load_dataset\n",
    "from fastcore.foundation import L\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.optim import lr_scheduler\n",
    "from fastprogress import progress_bar\n",
    "from torcheval.metrics import Mean, Metric\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "\n",
    "from diffusion_ai import *\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Configure logging and torch settings\n",
    "logging.disable(logging.WARNING)\n",
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "be41a4f8fd1e4263b58cd76f45fd32ec"
     ]
    },
    "id": "Qmx5IiAa1Prm",
    "outputId": "cef2ecb1-0c92-4cc2-8fff-ca41b2c10901"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_KEY, LABEL_KEY = 'image', 'label'\n",
    "NAME = \"fashion_mnist\"\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 512\n",
    "SIGMA_DATA = 0.66  # standard deviation of our tarining dataset\n",
    "dataset = load_dataset(NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13bIz_HB1Prw"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "#### <b>Sampling</b>\n",
    "\n",
    "Here we use `fid`, `kid` to compare the feature distribution of original data with the trained model's weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iP6hsvAn1Prw"
   },
   "outputs": [],
   "source": [
    "# evaluation model\n",
    "cnn_model = torch.load('models/inference.pkl')\n",
    "del(cnn_model[8])\n",
    "del(cnn_model[7])\n",
    "\n",
    "# pre-process\n",
    "@inplace\n",
    "def transformi(b): \n",
    "    b['image'] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b['image']]\n",
    "tds = dataset.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, BATCH_SIZE, num_workers=4)\n",
    "\n",
    "# first batch of our original data\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "\n",
    "sample_size = (512,1,32,32)\n",
    "ie = ImageEval(cnn_model, dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnYDWOaz1Pro",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### 1️⃣\n",
    "\n",
    "Let's implement our own `unet` model architecture and start training our diffusion model. This UNET model will not have any type of embeddings, class conditioning or attention mechanisms. Its a smiple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQCrySEq1Prn"
   },
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(batch):\n",
    "    batch[IMG_KEY] = [F.pad(TF.to_tensor(image), (2, 2, 2, 2)) * 2 - 1 for image in batch[IMG_KEY]]\n",
    "\n",
    "def compute_scalings(sigma):\n",
    "    # Compute scaling factors using karras approach\n",
    "    total_variance = sigma**2 + SIGMA_DATA**2\n",
    "    c_skip = SIGMA_DATA**2 / total_variance\n",
    "    c_out = sigma * SIGMA_DATA / total_variance.sqrt()\n",
    "    c_in = 1 / total_variance.sqrt()\n",
    "    return c_skip, c_out, c_in\n",
    "\n",
    "def noisify(images):\n",
    "    # Add noise to images using the Karras noise scheduler\n",
    "    device = images.device\n",
    "    sigma = (torch.randn([len(images)]) * 1.2 - 1.2).exp().to(images).reshape(-1, 1, 1, 1)\n",
    "    noise = torch.randn_like(images, device=device)\n",
    "    c_skip, c_out, c_in = compute_scalings(sigma)\n",
    "    noised_input = images + noise * sigma\n",
    "    target = (images - c_skip * noised_input) / c_out\n",
    "    return (noised_input * c_in, sigma.squeeze()), target\n",
    "\n",
    "def collate(batch):\n",
    "    # Custom collate function for DataLoader\n",
    "    return noisify(default_collate(batch)[IMG_KEY])\n",
    "\n",
    "def create_dataloader(dataset):\n",
    "    # Create a DataLoader for the given dataset\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN0yFx8_1Pro"
   },
   "outputs": [],
   "source": [
    "# Pre-process the data, add noise and create DataLoaders\n",
    "transformed_dataset = dataset.with_transform(transformi)\n",
    "dataloaders = DataLoaders(create_dataloader(transformed_dataset['train']), create_dataloader(transformed_dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "<b>---UNET ARCHITECTURE---</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJMsz-pg1Pro"
   },
   "outputs": [],
   "source": [
    "def unet_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMs_CzbL1Pro"
   },
   "outputs": [],
   "source": [
    "class UNetResBlock(nn.Module):\n",
    "    # Residual block for UNet\n",
    "    def __init__(self, in_channels, out_channels=None, kernel_size=3, activation=nn.SiLU, normalization=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "        self.convs = nn.Sequential(\n",
    "            unet_conv(in_channels, out_channels, kernel_size, act=activation, norm=normalization),\n",
    "            unet_conv(out_channels, out_channels, kernel_size, act=activation, norm=normalization)\n",
    "        )\n",
    "        self.identity_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else fc.noop\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convs(x) + self.identity_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm-9YIzA1Prp"
   },
   "outputs": [],
   "source": [
    "class SaveModule:\n",
    "    # Module that saves its output during the forward pass\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        self.saved = super().forward(x, *args, **kwargs)  # calls the next forward pass\n",
    "        return self.saved\n",
    "\n",
    "class SavedResBlock(SaveModule, UNetResBlock): pass\n",
    "class SavedConv(SaveModule, nn.Conv2d): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehNpY02X1Prp"
   },
   "outputs": [],
   "source": [
    "def down_block(in_channels, out_channels, add_downsample=True, num_layers=1):\n",
    "    # Create a downsampling block for the UNet\n",
    "    layers = [SavedResBlock(in_channels if i == 0 else out_channels, out_channels) for i in range(num_layers)]\n",
    "    if add_downsample:\n",
    "        layers.append(SavedConv(out_channels, out_channels, 3, stride=2, padding=1))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo_Ltvwo1Prp"
   },
   "outputs": [],
   "source": [
    "def upsample(nf):\n",
    "    # Create an upsampling layer for the UNet\n",
    "    return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dAZ-Mq1Prp"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    # Upsampling block for UNet\n",
    "    def __init__(self, in_channels, prev_out_channels, out_channels, add_upsample=True, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([\n",
    "            UNetResBlock((prev_out_channels if i == 0 else out_channels) + (in_channels if (i == num_layers - 1) else out_channels), out_channels)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.up_sample = upsample(out_channels) if add_upsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip_connections):\n",
    "        for resnet in self.resnets:\n",
    "            x = resnet(torch.cat([x, skip_connections.pop()], dim=1))\n",
    "        return self.up_sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjXCDunB1Prp"
   },
   "outputs": [],
   "source": [
    "# But when we use nn.ModuleList, we need a for loop to pass all the blocks.   \n",
    "class UNet(nn.Module):\n",
    "    # UNet architecture for image denoising\n",
    "    def __init__(self, in_channels=1, out_channels=1, feature_sizes=(224,448,672,896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, feature_sizes[0], kernel_size=3, padding=1)\n",
    "        self.down_blocks = nn.Sequential()\n",
    "        nf = feature_sizes[0]\n",
    "        for i in range(len(feature_sizes)):\n",
    "            ni = nf\n",
    "            nf = feature_sizes[i]\n",
    "            self.down_blocks.append(down_block(ni, nf, add_downsample=i!=len(feature_sizes)-1, num_layers=num_layers))\n",
    "        self.middle_block = UNetResBlock(feature_sizes[-1])\n",
    "        \n",
    "        rev_feature = list(reversed(feature_sizes))\n",
    "        nf = rev_feature[0]\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in range(len(feature_sizes)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_feature[i]\n",
    "            ni = rev_feature[min(i+1, len(feature_sizes)-1)]\n",
    "            self.up_blocks.append(UpBlock(ni, prev_nf, nf, add_upsample=i!=len(feature_sizes)-1, num_layers=num_layers+1))\n",
    "            \n",
    "        self.conv_out = unet_conv(feature_sizes[0], out_channels, activation=nn.SiLU, normalization=nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv_in(inp[0])\n",
    "        skip_connections = [x]\n",
    "        x = self.down_blocks(x)\n",
    "        skip_connections += [layer.saved for block in self.down_blocks for layer in block]\n",
    "        x = self.middle_block(x)\n",
    "        for block in self.up_blocks:\n",
    "            x = block(x, skip_connections)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssNQ6U_Q1Prp"
   },
   "outputs": [],
   "source": [
    "# Define model, optimizer, scheduler, and learner\n",
    "LR = 3e-3\n",
    "EPOCHS = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = EPOCHS * len(dataloaders.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=LR, total_steps=tmax)\n",
    "cbs = [DeviceCB(), MixedPrecision(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n",
    "model = UNet(in_channels=1, out_channels=1, feature_sizes=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dataloaders, nn.MSELoss(), lr=LR, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iUgNfWF1Prq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "learn.fit(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbZnbCC89VKR"
   },
   "source": [
    "<font face='monospace'>\n",
    "    \n",
    "The model that we get after training is basically a diffusion model with our own unet architecture, We can sample from it using any of our samplers. We trained it using noisified images, so we can sample from it using our either `ddpm` or `ddim` samplers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXvfptx21Prq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### 2️⃣\n",
    "\n",
    "#### <b>Timestep Model</b>\n",
    "This time let's create a model where we add sinusoidal noise at each time step. The below code is creating a **sinusoidal embedding** for timesteps. This technique is inspired by the positional encoding used in transformer models, where it helps the model to understand the order or position of elements (pixels in diffusion) in a sequence.\n",
    "\n",
    "In diffusion models, this type of embedding can provide information about which timestep (or noise level) an image is at during the denoising process. It helps the model to adapt its behavior based on how far along the diffusion process is, which is crucial for generating coherent outputs at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ4xrYNPGY2i"
   },
   "source": [
    "<font face='monospace'>\n",
    "    \n",
    "The `EmbUNetModel` is a U-Net architecture that incorporates timestep embeddings into its structure.\n",
    "\n",
    "- The main reason for using such a model in diffusion models is to condition the generation process on both local and global information about the image at different scales and timesteps. This allows for more controlled and coherent image generation as noise is progressively removed from an image over time.\n",
    "- In diffusion models, timestep embeddings are crucial because they allow the model to condition its predictions on the specific point in time during the denoising process. This is important because the amount and type of noise added to the images vary at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "06cd214311004aac81d20bf4701d0fcd"
     ]
    },
    "id": "UR_yBmIJ10fr",
    "outputId": "b6588ab2-df35-48ae-c743-ffb20e3da575"
   },
   "outputs": [],
   "source": [
    "IMG_KEY, LABEL_KEY = 'image','label'\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOOa3P2m10fs"
   },
   "outputs": [],
   "source": [
    "# using cosine noise scheduler\n",
    "def abar(t): return (t*math.pi/2).cos()**2\n",
    "def inv_abar(x): return x.sqrt().acos()*2/math.pi\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.rand(n,).to(x0).clamp(0,0.999)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[IMG_KEY])\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaCch6JY10ft"
   },
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[IMG_KEY] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[IMG_KEY]]\n",
    "\n",
    "tds = dataset.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_440RBY10fy"
   },
   "outputs": [],
   "source": [
    "# See the EmbUNetModel in diffusion_ai.diffusion.py file. This Unet has timestep embeddings and attention implemeted in it\n",
    "# In UNET model, we have also implemented attention channels in the mid blocks.\n",
    "lr = 1e-2\n",
    "epochs = 1\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRxtKTTM10fy",
    "outputId": "37d2b425-8519-45fe-aaa7-fd292213eb9f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "This code might not work sometimes because of `OutOfMemoryError`, simply try changing batch size to `32`, or `64`. It might take longer to run but its a small fix! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPUbFhYg10fz"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "#### <b>Sampling from the above model using ddim step</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig, clamp=True):\n",
    "    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n",
    "    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt())\n",
    "    if clamp: x_0_hat = x_0_hat.clamp(-1,1)\n",
    "    if bbar_t1<=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n",
    "    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n",
    "    x_t += sig * torch.randn(x_t.shape).to(x_t)\n",
    "    return x_0_hat,x_t\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(f, model, sz, steps, eta=1., clamp=True):\n",
    "    model.eval()\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100), clamp=clamp)\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds\n",
    "\n",
    "sample_size = (512, 1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsetkB1510f0",
    "outputId": "c45ae113-a5b8-46d1-e48c-1a5538bc5063"
   },
   "outputs": [],
   "source": [
    "# see diffusion_ai.diffusion.py file for this code\n",
    "preds = sample(ddim_step, model, sample_size, steps=100, eta=1.)\n",
    "s = (preds[-1]*2)\n",
    "s.min(),s.max(),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSDaDLU410f0",
    "outputId": "6f8ea26e-35fd-41a4-e0ea-9ef937c701a7"
   },
   "outputs": [],
   "source": [
    "show_images(s[:25].clamp(-1,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o682_ezz10f1",
    "outputId": "aad276eb-ea5d-4899-f12a-d676f3a1b788"
   },
   "outputs": [],
   "source": [
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSmOa8F710f1",
    "outputId": "ad08bf47-58b1-457a-8181-51baa5f89122"
   },
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sample_size, steps=100, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YvyxtwmImetW",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### 3️⃣\n",
    "\n",
    "<b>Conditional model</b><br>\n",
    "\n",
    "Here we try to add class embedding to the `unet` model.\n",
    "\n",
    "The `CondUNetModel` is similar to the `EmbUNetModel` in structure but includes an additional conditioning mechanism on class labels.\n",
    "\n",
    "While both models use timestep embeddings, the `CondUNetModel` extends this concept by also conditioning on class labels, making it suitable for a wider range of tasks that require fine-grained control over the output based on categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5wfXCMl10f2"
   },
   "outputs": [],
   "source": [
    "def collate_ddpm(b):\n",
    "    # Here we try to group both images and labels\n",
    "    b = default_collate(b)\n",
    "    (xt,t),eps = noisify(b[IMG_KEY])\n",
    "    return (xt,t,b[LABEL_KEY]),eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZR6wkPv10f2"
   },
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): \n",
    "    b[IMG_KEY] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[IMG_KEY]]\n",
    "\n",
    "tds = dataset.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t,c),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYpKdVtK10f7"
   },
   "outputs": [],
   "source": [
    "class CondUNetModel(nn.Module):\n",
    "    def __init__( self, n_classes, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t,c = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7XkSQsp10f7"
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = CondUNetModel(10, in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPo3u_l910f7",
    "outputId": "c9be5fa4-74de-42cf-a88b-fbcb0eca4345"
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoKxh9ut10f8"
   },
   "outputs": [],
   "source": [
    "sz = (256,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQQokIT010f8",
    "outputId": "8aa30591-826d-4e22-e84c-c352d9478964"
   },
   "outputs": [],
   "source": [
    "lbls = dataset['train'].features[yl].names\n",
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhjfW8R310f8",
    "outputId": "465a72c0-5e40-43cc-812c-aa65607e77de"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "# see in diffusion_ai.diffusion.py file.\n",
    "preds = cond_sample(cid, ddim_step, model, sz, steps=100, eta=1.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHHfKML_10f8",
    "outputId": "3e923c77-de93-40c6-a359-e612e10a1ea4"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "preds = cond_sample(cid, ddim_step, model, sz, steps=100, eta=0.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvZ7qnPv2Qzu"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### 4️⃣\n",
    "\n",
    "<b>Variational Auto Encoder</b>\n",
    "\n",
    "_NOTE_: The goal of VAE is to encode a normal image from higher dimension into a latent image of lowe dimensions. Then we reconstruct the input images but decoding the latents hence we set both inputs and targets to be the same flattened images.\n",
    "\n",
    "Here we just see how latents can be used as inputs to the unet model instead of noisy images. **WHY?** Because we can have input images of any resolution, and training UNET models for higher resolution is very costly and time consuming. So, we reduce the dimension of the image and use them to save time, cost, space, etc. Just make life simpler!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA8OYO0M2Q0A"
   },
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 64\n",
    "dsd = load_dataset(name)\n",
    "\n",
    "@inplace\n",
    "def transformi(b):\n",
    "    img = [TF.to_tensor(o).flatten() for o in b[xl]]\n",
    "    b[yl] = b[xl] = img\n",
    "    # This means that both the inputs (xl) and targets (yl)\n",
    "    # for the model are now the same flattened images.\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)\n",
    "\n",
    "dl = dls.valid\n",
    "xb,yb = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adlIZkDe2Q0A"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "- **ni**: Input dimension (number of features)\n",
    "- **nh**: Hidden layer dimension (number of neurons in each hidden layer)\n",
    "- **nl**: Latent space dimension (dimensionality of the compressed representation)\n",
    "\n",
    "These are the dimension variables used in a simple autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtGLbAgD2Q0B"
   },
   "outputs": [],
   "source": [
    "ni,nh,nl = 784,400,200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ38GkhC2Q0C"
   },
   "outputs": [],
   "source": [
    "def lin(ni, nf, act=nn.SiLU, norm=nn.BatchNorm1d, bias=True):\n",
    "    layers = nn.Sequential(nn.Linear(ni, nf, bias=bias))\n",
    "    if act : layers.append(act())\n",
    "    if norm: layers.append(norm(nf))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIXXY0Wv2Q0C"
   },
   "outputs": [],
   "source": [
    "def init_weights(m, leaky=0.):\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)):\n",
    "        init.kaiming_normal_(m.weight, a=leaky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = partial(init_weights, leaky=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCSlJ7xR2Q0D"
   },
   "outputs": [],
   "source": [
    "class Autoenc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(lin(ni, nh), lin(nh, nh), lin(nh, nl))\n",
    "        self.dec = nn.Sequential(lin(nl, nh), lin(nh, nh), lin(nh, ni, act=None))\n",
    "        iw(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        return self.dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQQONpEo2Q0E"
   },
   "outputs": [],
   "source": [
    "lr = 3e-2\n",
    "epochs = 20\n",
    "tmax = epochs * len(dls.train)\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = Autoenc()\n",
    "learn = Learner(model, dls, nn.BCEWithLogitsLoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCRO3-Ph2Q0E",
    "outputId": "1418215f-02ed-4ad1-d235-6046e019e60a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i6QzKX62Q0E"
   },
   "source": [
    "<font face='monospace'>\n",
    "After training we get an encdoer model which can be used to compress our dataset and use the compressed version for training.\n",
    "let's try sampling from our autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNbjee0e2Q0F"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): t = to_cpu(model(xb).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huVMAYIc2Q0F",
    "outputId": "4bf77367-7060-4572-df5c-ff130be8fcc5"
   },
   "outputs": [],
   "source": [
    "# original\n",
    "show_images(xb[:9].reshape(-1,1,28,28), imsize=1.5, title='Original');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqR2rR8c2Q0F",
    "outputId": "e572d58f-d6dd-46e9-8147-20d5045188be"
   },
   "outputs": [],
   "source": [
    "# generated\n",
    "show_images(t[:9].reshape(-1,1,28,28).sigmoid(), imsize=1.5, title='Autoenc');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGzj0cUhqbsP"
   },
   "source": [
    "<font face='monospace' color='#800080'>\n",
    "<div class=\"alert alert-info\">\n",
    "  <i class=\"fas fa-lightbulb\"></i> latents are not the images themselves, they are tensors, but are valuable for many computer vision tasks due to their ability to encapsulate important aspects of the data in a more efficient form.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "**Variational Autoencoder (VAE)**: An extension of the autoencoder architecture that introduces probabilistic elements into the latent space. Here are the key differences:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - The VAE's encoder maps the input data to a lower-dimensional latent space.\n",
    "   - Instead of directly producing a fixed latent representation, it generates two vectors:\n",
    "     - **Mean (`mu`)**: Represents the center of a Gaussian distribution in the latent space.\n",
    "     - **Log Variance (`lv`)**: Determines the spread or uncertainty of the distribution.\n",
    "   - By combining `mu` and `lv`, the encoder creates a probabilistic representation of the input data.\n",
    "\n",
    "2. **Latent Space**:\n",
    "   - The latent space in a VAE is continuous and probabilistic.\n",
    "   - It allows for sampling from the learned distribution, enabling generative capabilities.\n",
    "   - Each point in the latent space corresponds to a potential data point.\n",
    "\n",
    "3. **Decoder**:\n",
    "   - The decoder takes a sampled latent vector as input.\n",
    "   - It reconstructs the original input data from this probabilistic representation.\n",
    "   - The decoder learns to generate realistic data points by sampling from the latent space.\n",
    "\n",
    "**Use Cases**:\n",
    "  - Image generation, Data denoising, Learning meaningful representations for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKiCgSTb2Q0G"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(lin(ni, nh), lin(nh, nh))\n",
    "        self.mu,self.lv = lin(nh, nl, act=None),lin(nh, nl, act=None)\n",
    "        self.dec = nn.Sequential(lin(nl, nh), lin(nh, nh), lin(nh, ni, act=None))\n",
    "        iw(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        mu,lv = self.mu(x),self.lv(x)\n",
    "        z = mu + (0.5*lv).exp()*torch.randn_like(lv)\n",
    "        return self.dec(z),mu,lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWecIMKu2Q0H"
   },
   "outputs": [],
   "source": [
    "# Kullback-Leibler divergence loss formula is used to calculate the divergence b/w \n",
    "# model's learned distribution space and the dataset's original distribution\n",
    "\n",
    "def kld_loss(inp, x):\n",
    "    x_hat,mu,lv = inp\n",
    "    return -0.5 * (1 + lv - mu.pow(2) - lv.exp()).mean()\n",
    "\n",
    "def bce_loss(inp, x): \n",
    "    return F.binary_cross_entropy_with_logits(inp[0], x)\n",
    "\n",
    "def vae_loss(inp, x):\n",
    "    return kld_loss(inp, x) + bce_loss(inp,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5vFeEAr2Q0H"
   },
   "outputs": [],
   "source": [
    "class FuncMetric(Mean):\n",
    "    def __init__(self, fn, device=None):\n",
    "        super().__init__(device=device)\n",
    "        self.fn = fn\n",
    "\n",
    "    def update(self, inp, targets):\n",
    "        self.weighted_sum += self.fn(inp, targets)\n",
    "        self.weights += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9A9Zgds2Q0H"
   },
   "outputs": [],
   "source": [
    "metrics = MetricsCB(kld=FuncMetric(kld_loss), bce=FuncMetric(bce_loss))\n",
    "opt_func = partial(optim.Adam, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "416siSx12Q0I"
   },
   "outputs": [],
   "source": [
    "lr = 3e-2\n",
    "epochs = 20\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), metrics, BatchSchedCB(sched), MixedPrecision()]\n",
    "model = VAE()\n",
    "learn = Learner(model, dls, vae_loss, lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1ajLBEK2Q0I",
    "outputId": "f8d41d4d-58a6-4598-9b32-9f674ddf31ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP9s_1kA2Q0I"
   },
   "source": [
    "<font face='monospace'>**Sampling** from our **VAE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyDCmqUa2Q0I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): t,mu,lv = to_cpu(model(xb))\n",
    "t = t.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU44_nd32Q0J",
    "outputId": "83ee8e4a-015f-4625-ed92-1460746ccc24"
   },
   "outputs": [],
   "source": [
    "show_images(xb[:9].reshape(-1,1,28,28), imsize=1.5, title='Original');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gapSauaT2Q0J",
    "outputId": "401142fa-d9d2-4651-c719-aeb9dd8a3d8c"
   },
   "outputs": [],
   "source": [
    "show_images(t[:9].reshape(-1,1,28,28).sigmoid(), imsize=1.5, title='VAE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lTUE8Oo2Q0J",
    "outputId": "092b0203-9efb-4da9-f3c5-cdca42e4b051"
   },
   "outputs": [],
   "source": [
    "# using normal distribution noise to sample new data\n",
    "\n",
    "noise = torch.randn(16, nl)\n",
    "with torch.no_grad(): \n",
    "    ims = model.dec(noise).sigmoid()\n",
    "show_images(ims.reshape(-1, 1, 28, 28), imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'> Now all we have to do is just try using these latents as the input data and train our model. This makes our model more flexible. We will just see how to put these latents into the model. The rest of the procedure remains same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxO1GBTG2rAl"
   },
   "source": [
    "<font face='monospace'>\n",
    "\n",
    "### 5️⃣\n",
    "\n",
    "Using latents as training data for a diffusion model. This is normally done for colour images and not black and white. The code looks something like this.\n",
    "\n",
    "```python\n",
    "\n",
    "# Directory setup, data download, and data extraction\n",
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path = path_data / 'bedroom'\n",
    "url = 'https://s3.amazonaws.com/fast-ai-imageclas/bedroom.tgz'\n",
    "if not path.exists():\n",
    "    path_zip = fc.urlsave(url, path_data)\n",
    "    shutil.unpack_archive('data/bedroom.tgz', 'data')\n",
    "\n",
    "# Create a dataset\n",
    "def to_img(f): \n",
    "    return read_image(f, mode=ImageReadMode.RGB) / 255\n",
    "\n",
    "class ImagesDS:\n",
    "    def __init__(self, spec):\n",
    "        self.path = Path(path)\n",
    "        self.files = glob(str(spec), recursive=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return to_img(self.files[i])[:, :256, :256]\n",
    "        \n",
    "ds = ImagesDS(path / '**/*.jpg')\n",
    "dl = DataLoader(ds, batch_size=64, num_workers=defaults.cpus)\n",
    "xb = next(iter(dl))\n",
    "show_images(xb[:16], imsize=2)\n",
    "\n",
    "# Using Hugging Face model (Variational Autoencoder)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)\n",
    "xe = vae.encode(xb.cuda())\n",
    "xs = xe.latent_dist.mean[:16]\n",
    "show_images(((xs[:16, :3]) / 4).sigmoid(), imsize=2)\n",
    "xd = to_cpu(vae.decode(xs))\n",
    "show_images(xd['sample'].clamp(0, 1), imsize=2)\n",
    "\n",
    "# Create a memory-mapped array for latents\n",
    "mmpath = Path('data/bedroom/data.npmm')\n",
    "if not mmpath.exists():\n",
    "    a = np.memmap(mmpath, np.float32, mode='w+', shape=(303125, 4, 32, 32))\n",
    "    i = 0\n",
    "    for b in progress_bar(dl):\n",
    "        n = len(b)\n",
    "        a[i:i + n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n",
    "        i += n\n",
    "    a.flush()\n",
    "    del a\n",
    "\n",
    "# collate function\n",
    "def collate(b):\n",
    "    return noisify(default_collate(b)*0.2)\n",
    "\n",
    "# Split latents into training and validation sets\n",
    "lats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=(303125, 4, 32, 32))\n",
    "tds = lats[:len(lats) // 10 * 9]\n",
    "vds = lats[len(lats) // 10 * 9:]\n",
    "bs = 128\n",
    "dls = DataLoaders(*get_dls(tds, vds, bs=bs, num_workers=defaults.cpus, collate_fn=collate))\n",
    "(xt, t), eps = b = next(iter(dls.train))\n",
    "\n",
    "# Initialize DDPM model\n",
    "def init_ddpm(model):\n",
    "    for o in model.downs:\n",
    "        for p in o.resnets:\n",
    "            p.conv2[-1].weight.data.zero_()\n",
    "\n",
    "    for o in model.ups:\n",
    "        for p in o.resnets:\n",
    "            p.conv2[-1].weight.data.zero_()\n",
    "\n",
    "lr = 3e-3\n",
    "epochs = 25\n",
    "opt_func = partial(optim.AdamW, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "\n",
    "# Create DDPM model using the embedded unet architecture\n",
    "model = EmbUNetModel(in_channels=4, out_channels=4, nfs=(128, 256, 512, 768), num_layers=2, attn_start=1, attn_chans=16)\n",
    "init_ddpm(model)\n",
    "learn = Learner(model, dls, MSELossFlat(), lr=lr, cbs=cbs, opt_func=opt_func)\n",
    "learn.fit(epochs)\n",
    "\n",
    "# Generate samples\n",
    "sample_size = (16, 4, 32, 32)\n",
    "preds = sample(ddim_step, model, sample_size, steps=100, eta=1.0, clamp=False)\n",
    "S = preds[-1]\n",
    "\n",
    "# Reconstruct images from latents\n",
    "with torch.no_grad():\n",
    "    pd = to_cpu(vae.decode(S.cuda()))\n",
    "show_images(pd['sample'][:9].clamp(0, 1), im\n",
    "```\n",
    "\n",
    "<br>\n",
    "Note that this is again normally done for color images and not needed for grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='monospace'>\n",
    "<div class=\"alert alert-warning\">\n",
    "  <i class=\"fas fa-exclamation-circle\"></i>&nbsp;<strong>Warning</strong><br>\n",
    "  The minimum GPU memory required depends on your specific model and dataset.\n",
    "  For most of the above models, GPUs with at least 4GB VRAM are recommended.\n",
    "  NVIDIA T4 - A single <code>T4 GPU</code>, <code>16GB VRAM</code>, <code>8 CPUs</code> having a speed of <code>65 TFLOPs</code> would still crash if used for above models without proper strategy.  \n",
    "\n",
    "  Try to train the above models if the below requirements are satisfied. Or else just understand how the code works.\n",
    "  - GPUs capable of running the above models\n",
    "      - <code>NVIDIA P100</code> ~16GB VRAM\n",
    "      - <code>NVIDIA V100</code>: ~16GB to 32GB VRAM\n",
    "      - <code>NVIDIA A100</code>: ~40GB to 80GB VRAM\n",
    "  - Using Cloud GPUs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
